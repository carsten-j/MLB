\section{Occam's razor with \texorpdfstring{$\kl$}{kl} inequality (30 points) [Yevgeny]}

I have not be able to provide a direct proof of Occam's razor with $\kl$ inequality.
As an alternative I used a "backward" approach going from the desired result moving backwards to the assumptions of the theorem. Using this approach I end up with the following in-equality

\begin{equation}\label{eq:assumption}
\P(\kl(\hat{L}(h,S)\|L(h)) \geq \varepsilon) \leq e^{-n\varepsilon}
\end{equation}
that should hold for any $\varepsilon > 0$. It looks like a $\kl$-version of Chernoff's bound\footnote{See \cite{mitzenmacherProbabilityComputingRandomized2005} for Chernoff bounds.} but I have not be able to proof that it is correct. Assuming eq. \ref{eq:assumption} is correct the proof goes like this.

\begin{theorem}[Occam's kl-razor inequality]
Let $S$ be an i.i.d. sample of $n$ points, let $\ell$ be a loss function bounded in the interval $[0, 1]$, let $\HH$ be countable and let $\pi(h)$ be such that it is independent of the sample $S$ and satisfies $\pi(h) \geq 0$ for all $h$ and $\sum_{h\in\mathcal{H}} \pi(h) \leq 1$. Let $\delta \in (0,1)$. Then
\begin{equation*}
\P\left(\exists h \in \HH : \kl(\hat{L}(h, S)\|L(h)) \geq \frac{\ln \frac{1}{\pi(h)\delta}}{n}\right) \leq \delta.
\end{equation*}
\end{theorem}

\begin{proof}
Define for each hypothesis $h$:

\begin{equation*}
\varepsilon_h = \frac{\ln \frac{1}{\pi(h)\delta}}{n}
\end{equation*}

Using eq. \ref{eq:assumption} this gives us:
\begin{equation*}
\P\left(\kl(\hat{L}(h,S)\|L(h)) \geq \frac{\ln \frac{1}{\pi(h)\delta}}{n}\right) \leq e^{-n \cdot \frac{\ln \frac{1}{\pi(h)\delta}}{n}} 
= e^{-\ln \frac{1}{\pi(h)\delta}} 
= \pi(h)\delta
\end{equation*}

Now we apply the union bound over all $h \in \HH$:

\begin{align*}
\P\left(\exists h \in \HH : \kl(\hat{L}(h,S)\|L(h)) \geq \frac{\ln \frac{1}{\pi(h)\delta}}{n}\right) &\leq \sum_{h \in \HH} \P\left(\kl(\hat{L}(h,S)\|L(h)) \geq \frac{\ln \frac{1}{\pi(h)\delta}}{n}\right) \\
&\leq \sum_{h \in \HH} \pi(h)\delta 
= \delta \sum_{h \in \HH} \pi(h) 
\leq \delta \cdot 1 
= \delta
\end{align*}

where the second last inequality follows from the condition that $\sum_{h \in \mathcal{H}} \pi(h) \leq 1$.
\end{proof}


\subsection*{Importance of $\pi(h)$ being independent of $S$}

The critical step where we use the independence of $\pi(h)$ from the sample $S$ is when applying the union bound. If $\pi(h)$ were to depend on $S$, we could not treat it as a fixed quantity when calculating the probability. Without independence, $\pi(h)$ becomes a random variable that depends on the same sample $S$ we are using to compute $\hat{L}(h,S)$.


\begin{corollary}
Under the assumptions of Theorem 3.38 (Occam's kl-razor inequality), the following holds:
\begin{equation}
\P\left(\exists h \in \HH : L(h) \geq \hat{L}(h, S) + \sqrt{\frac{2\hat{L}(h,S)\ln \frac{1}{\pi(h)\delta}}{n}} + \frac{2\ln \frac{1}{\pi(h)\delta}}{n}\right) \leq \delta.
\end{equation}
\end{corollary}

\begin{proof}
From the above Theorem, with probability at least $1-\delta$, for all $h \in \HH$:
$$\kl(\hat{L}(h,S)\|L(h)) < \frac{\ln\frac{1}{\pi(h)\delta}}{n}$$

For $p, q \in [0,1]$ with $p \leq q$, we can use the following lower bound on KL-divergence:
\begin{equation*}
\frac{(q-p)^2}{2q} \leq \kl(p||q).
\end{equation*}
This is from corollary 2.31 (Reﬁned Pinsker’s inequality) in the lecture notes \cite{seldinMachineLearningScience2025}. Here we are interested in the case where $\hat{L}(h,S) \leq L(h)$, we can apply this with $p = \hat{L}(h,S) := \hat{L}$ and $q = L(h) := L$, where the last equality is simplification of the notation.

$$\frac{(L-\hat{L})^2}{2L} \leq \text{kl}(\hat{L}||L)$$

Now theorem 3.38 give us, with probability at least $1-\delta$:
$$\frac{(L-\hat{L})^2}{2L} < \frac{\ln\frac{1}{\pi(h)\delta}}{n}$$

This is a quadratic inequality in $L$, that we can re-write to:
$$L^2 - L\left(2\hat{L} + \frac{2\ln\frac{1}{\pi(h)\delta}}{n}\right) + \hat{L}^2 < 0$$

Using the quadratic formula, the solutions to $aL^2 + bL + c = 0$ with $$a = 1, b = -\left(2\hat{L} + \frac{2\ln\frac{1}{\pi(h)\delta}}{n}\right), \text{and } c = \hat{L}^2$$ are:

$$L = \hat{L} + \frac{\ln\frac{1}{\pi(h)\delta}}{n} \pm \sqrt{\frac{2\hat{L}\ln\frac{1}{\pi(h)\delta}}{n} + \frac{(\ln\frac{1}{\pi(h)\delta})^2}{n^2}}$$

Please see \ref{sec:derivation} for details on calculating the roots. For a quadratic inequality of the form $aL^2 + bL + c < 0$ with $a > 0$, the solution is between the two roots. We are looking for an upper bound for $L$, which will be the larger root:

$$L < \hat{L} + \frac{\ln\frac{1}{\pi(h)\delta}}{n} + \sqrt{\frac{2\hat{L}\ln\frac{1}{\pi(h)\delta}}{n} + \frac{(\ln\frac{1}{\pi(h)\delta})^2}{n^2}}$$

Using that for non-negative $a$ and $b$ we have $\sqrt{a+b} \leq \sqrt{a} + \sqrt{b}$:

$$L < \hat{L} + \frac{\ln\frac{1}{\pi(h)\delta}}{n} + \sqrt{\frac{2\hat{L}\ln\frac{1}{\pi(h)\delta}}{n}} + \frac{\ln\frac{1}{\pi(h)\delta}}{n} = \hat{L} + \sqrt{\frac{2\hat{L}\ln\frac{1}{\pi(h)\delta}}{n}} + \frac{2\ln\frac{1}{\pi(h)\delta}}{n}$$

Hence with probability at least $1-\delta$, for all $h \in \HH$:
$$L(h) < \hat{L}(h,S) + \sqrt{\frac{2\hat{L}(h,S)\ln\frac{1}{\pi(h)\delta}}{n}} + \frac{2\ln\frac{1}{\pi(h)\delta}}{n}$$

Using the complement event:
$$\P\left(\exists h \in \mathcal{H} : L(h) \geq \hat{L}(h,S) + \sqrt{\frac{2\hat{L}(h,S)\ln\frac{1}{\pi(h)\delta}}{n}} + \frac{2\ln\frac{1}{\pi(h)\delta}}{n}\right) \leq \delta$$

which is exactly the statement of the Corollary.
\end{proof}

\subsection*{Discussion  of the Corollary}

This corollary provides important advantages over the original KL-divergence formulation:

\begin{enumerate}
\item It provides an explicit upper bound on the true loss $L(h)$ in terms of the empirical loss $\hat{L}(h,S)$
\item It clearly shows the convergence rate through the terms $\sqrt{\frac{2\hat{L}(h,S)\ln \frac{1}{\pi(h)\delta}}{n}}$ and $\frac{2\ln \frac{1}{\pi(h)\delta}}{n}$
\item The first term scales with $\sqrt{\frac{\hat{L}(h,S)}{n}}$, showing faster convergence for hypotheses with lower empirical error
\end{enumerate}
