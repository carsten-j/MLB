@book{boydConvexOptimization2004a,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  date = {2004-03-08},
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511804441},
  url = {https://www.cambridge.org/core/product/identifier/9780511804441/type/book},
  urldate = {2025-04-25},
  isbn = {978-0-521-83378-3 978-0-511-80444-1},
  keywords = {MLB}
}

@book{coverElementsInformationTheory2005,
  title = {Elements of {{Information Theory}}},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  date = {2005-09-16},
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/047174882X},
  url = {https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X},
  urldate = {2025-04-25},
  isbn = {978-0-471-24195-9 978-0-471-74882-3},
  langid = {english},
  keywords = {MLB}
}

@online{garrigosHandbookConvergenceTheorems2024,
  title = {Handbook of {{Convergence Theorems}} for ({{Stochastic}}) {{Gradient Methods}}},
  author = {Garrigos, Guillaume and Gower, Robert M.},
  date = {2024-03-09},
  eprint = {2301.11235},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2301.11235},
  url = {http://arxiv.org/abs/2301.11235},
  urldate = {2025-04-25},
  abstract = {This is a handbook of simple proofs of the convergence of gradient and stochastic gradient descent type methods. We consider functions that are Lipschitz, smooth, convex, strongly convex, and/or Polyak-\{\textbackslash L\}ojasiewicz functions. Our focus is on ``good proofs'' that are also simple. Each section can be consulted separately. We start with proofs of gradient descent, then on stochastic variants, including minibatching and momentum. Then move on to nonsmooth problems with the subgradient method, the proximal gradient descent and their stochastic variants. Our focus is on global convergence rates and complexity rates. Some slightly less common proofs found here include that of SGD (Stochastic gradient descent) with a proximal step, with momentum, and with mini-batching without replacement.},
  pubstate = {prepublished},
  keywords = {Mathematics - Optimization and Control,MLB}
}

@book{mitzenmacherProbabilityComputingRandomized2005,
  title = {Probability and {{Computing}}: {{Randomized Algorithms}} and {{Probabilistic Analysis}}},
  shorttitle = {Probability and {{Computing}}},
  author = {Mitzenmacher, Michael and Upfal, Eli},
  date = {2005-01-31},
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511813603},
  url = {https://www.cambridge.org/core/product/identifier/9780511813603/type/book},
  urldate = {2025-04-28},
  isbn = {978-0-521-83540-4 978-0-511-81360-3},
  keywords = {MLB}
}

@book{mohriFoundationsMachineLearning2018,
  title = {Foundations of {{Machine Learning}}},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  date = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {MIT Press},
  location = {Cambridge, MA},
  isbn = {978-0-262-03940-6 978-0-262-35136-2},
  langid = {english},
  pagetotal = {1},
  keywords = {MLB}
}

@misc{seldinMachineLearningScience2025,
  title = {Machine {{Learning The Science}} of {{Selection}} under {{Uncertainty}}},
  author = {Seldin, Yevgeny},
  date = {2025-04-22},
  url = {https://sites.google.com/site/yevgenyseldin/teaching},
  keywords = {MLB}
}

@online{wuSplitklPACBayessplitklInequalities2022,
  title = {Split-Kl and {{PAC-Bayes-split-kl Inequalities}} for {{Ternary Random Variables}}},
  author = {Wu, Yi-Shan and Seldin, Yevgeny},
  date = {2022},
  doi = {10.48550/ARXIV.2206.00706},
  url = {https://arxiv.org/abs/2206.00706},
  urldate = {2025-04-27},
  abstract = {We present a new concentration of measure inequality for sums of independent bounded random variables, which we name a split-kl inequality. The inequality is particularly well-suited for ternary random variables, which naturally show up in a variety of problems, including analysis of excess losses in classification, analysis of weighted majority votes, and learning with abstention. We demonstrate that for ternary random variables the inequality is simultaneously competitive with the kl inequality, the Empirical Bernstein inequality, and the Unexpected Bernstein inequality, and in certain regimes outperforms all of them. It resolves an open question by Tolstikhin and Seldin [2013] and Mhammedi et al. [2019] on how to match simultaneously the combinatorial power of the kl inequality when the distribution happens to be close to binary and the power of Bernstein inequalities to exploit low variance when the probability mass is concentrated on the middle value. We also derive a PAC-Bayes-split-kl inequality and compare it with the PAC-Bayes-kl, PAC-Bayes-Empirical-Bennett, and PAC-Bayes-Unexpected-Bernstein inequalities in an analysis of excess losses and in an analysis of a weighted majority vote for several UCI datasets. Last but not least, our study provides the first direct comparison of the Empirical Bernstein and Unexpected Bernstein inequalities and their PAC-Bayes extensions.},
  pubstate = {prepublished},
  keywords = {MLB}
}
