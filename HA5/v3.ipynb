{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b838954f",
   "metadata": {},
   "source": [
    "Below is a “cook-book” style plan showing how you can reproduce Figure 2 a of Thiemann et al. (2017) with Python and scikit-learn.\n",
    "Everything is written as a sequence of clearly separated steps so that you can translate it almost literally into code (Jupyter, PyCharm, …).\n",
    "Only standard scientific-Python packages are needed: numpy, scikit-learn, scipy, matplotlib.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "0.  FOLDER STRUCTURE AND REPRODUCIBILITY\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "│ ionosphere_demo/\n",
    "│   ├── data/                # raw ionosphere.data from UCI\n",
    "│   ├── iono_experiment.py   # your main script / notebook\n",
    "│   └── utils.py             # helper routines (optional)\n",
    "\n",
    "• Fix the random seed at the beginning of every run (np.random.seed, random.seed).\n",
    "• Use float64 everywhere (numpy default).\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "DATA PREPARATION  (IONOSPHERE)\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "1.1  Download “ionosphere.data” from UCI ML repository.\n",
    "1.2  Convert the “g / b” label to {+1, –1}.\n",
    "1.3  z-score each column (StandardScaler in scikit-learn).\n",
    "1.4  Split as in the paper (Table 1):\n",
    "\n",
    "n      = 200   (training set S)\n",
    "n_test = 150   (test set T)\n",
    "\n",
    "Randomly shuffle the 350 available points, take the first 200 for S\n",
    "and the remaining 150 for T.  Store the permutation so the same split\n",
    "is used for all values of m.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "2.  REFERENCE (STRONG) SVM TUNED BY 5-FOLD CV\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "2.1  Grid for C          : 10^{-3,…,+3}\n",
    "2.2  Grid for γ (RBF)    : Jaakkola heuristic as in the paper\n",
    "• Compute G_i = min_{j: y_j≠y_i} ‖x_i – x_j‖.\n",
    "• γ_J = 1 / (2 median(G)^2).\n",
    "• grid = γ_J · 10^{–4, –2, 0, +2, +4}.\n",
    "\n",
    "2.3  sklearn.model_selection.GridSearchCV with\n",
    "SVC(kernel='rbf', probability=False, cache_size=500, shrinking=True).\n",
    "\n",
    "2.4  Fit on the 200 training points, keep the best estimator, call it “CV-SVM”.\n",
    "2.5  Evaluate zero-one loss on the 150 test points → L_CV (≈ 0.06 in the paper).\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "3.  BUILD THE FINITE HYPOTHESIS SET  H\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "Fixed constants for Fig. 2 a:\n",
    "\n",
    "d  = 34                        # number of features after preprocessing  \n",
    "r  = d + 1 = 35                # size of each weak-SVM training subset  \n",
    "m  ∈ {1, 2, 4, 8, 16, 32, 64, 128}   # values along the x-axis in Fig. 2 a  \n",
    "For each value of m do:\n",
    "\n",
    "3.1  Loop j = 1 … m\n",
    "• Draw a subset S_j (size r) without replacement from the 200 training\n",
    "indices.\n",
    "• Validation set V_j = S \\ S_j  (size n–r = 165).\n",
    "• Choose γ_j uniformly at random from the γ-grid defined in step 2.\n",
    "• Fix C = 1  (paper reports that the margin parameter has little influence\n",
    "for such tiny training sets).\n",
    "• Train an SVC on (X[S_j], y[S_j]) with kernel='rbf', C=1, gamma=γ_j.\n",
    "• Store the fitted SVM, its training indices S_j and γ_j.\n",
    "\n",
    "3.2  After the loop you have m trained weak classifiers h_1,…,h_m.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "4.  COLLECT VALIDATION LOSSES  L̂_val(h_j)\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "For each weak SVM h_j:\n",
    "\n",
    "L̂_val(h_j) =  (# misclassified points in V_j) / |V_j|   ∈ [0,1].\n",
    "Store the vector L̂ = (L̂_1,…,L̂_m) and also n_val = n – r = 165 (will be needed).\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "5.  MINIMISE THE PAC-BAYES-λ BOUND\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "Notation reminder (Section 3 in the paper, with the “finite H” constants):\n",
    "\n",
    "π(h) = 1/m                   (uniform prior)  \n",
    "δ    = 0.05                   (fixed)  \n",
    "n    = n_val = 165           (validation set size)  \n",
    "L̂    = vector of validation losses  \n",
    "Alternating minimisation loop\n",
    "———————————————\n",
    "Initialise\n",
    "ρ^{(0)} = (1/m,…,1/m),     λ^{(0)} = 0.5.\n",
    "Repeat until convergence (≤ 1e-6 relative change or ≤20 iterations):\n",
    "\n",
    "(1) ρ-update    (use Eq. (7))\n",
    "ρ_i ∝ π_i · exp( – λ n L̂_i )        → normalise\n",
    "\n",
    "    KL = Σ ρ_i log(m ρ_i)                # because π_i = 1/m\n",
    "(2) λ-update    (use Eq. (8))\n",
    "A = Σ ρ_i L̂_i                       # empirical risk under ρ\n",
    "B = KL + ln( 2 √n / δ )              # helper\n",
    "λ  =  2 / ( √( 2 n A / B ) + 1 )    # equivalent to Eq. (8)\n",
    "\n",
    "    Clip λ to ( √( ln(2√n/δ) / n ) , 1.0 ) for numerical safety.\n",
    "Return final ρ, λ.\n",
    "\n",
    "Compute the value of the PAC-Bayes-λ bound (Eq. (13)) and keep it for plotting.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "6.  PREDICTION ON THE HELD-OUT TEST SET (150 points)\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "Two variants:\n",
    "\n",
    "6.1  Randomised classifier (optional)\n",
    "Sample one weak SVM from ρ for every test point.\n",
    "\n",
    "6.2  ρ-WEIGHTED MAJORITY VOTE (what Figure 2 uses)\n",
    "For each test point x:\n",
    "logit = Σ_{j=1}^m ρ_j · sign( h_j(x) )\n",
    "ŷ = +1   if logit > 0   else –1\n",
    "Loss_MV = (# errors) / 150.\n",
    "\n",
    "6.3  Record the time spent in:\n",
    "• generating all subsets + training all weak SVMs\n",
    "• alternating minimisation\n",
    "(Use Python’s time.perf_counter())\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "7.  PLOT FIGURE 2 a\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "Create four 1-D curves (parameterised by m):\n",
    "\n",
    "•  “Our Method”    : test loss of majority vote  \n",
    "•  “Bound”         : PAC-Bayes-kl bound on E_ρ[L(h)] (see below)  \n",
    "•  “CV SVM”        : constant horizontal line = L_CV  \n",
    "•  time curves     : t_m (PAC-Bayes pipeline) and t_CV (cross-validation)  \n",
    "PAC-Bayes-kl bound\n",
    "Use Theorem 2 with n = n_val, replace empirical loss E_ρ[L̂] by A.\n",
    "Numerically invert kl(a || ·) with bisection (scipy.optimize.brentq).\n",
    "The result BOUND(m) is deterministic once ρ and A are known.\n",
    "\n",
    "Axes and style\n",
    "x-axis: m (log scale 10^0 … 10^2 in figure)\n",
    "left y-axis: test loss curves\n",
    "right y-axis: running times (secondary axis)\n",
    "\n",
    "Matplotlib convenience:\n",
    "\n",
    "fig, ax1 = plt.subplots()  \n",
    "ax2 = ax1.twinx()  \n",
    "ax1.plot(m_vals, loss_PB, 'k', label='Our Method')  \n",
    "…  \n",
    "────────────────────────────────────────────────────────────────────────\n",
    "8.  RUN  AND  CHECK\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "• The black solid line (“Our Method”) should dive from ≈0.30 down to ≈0.08\n",
    "and reach (or slightly beat) the red horizontal CV-SVM line ≈ 0.06.\n",
    "• The bound (blue) tracks the black curve from above.\n",
    "• The dashed black runtime curve should stay far below the horizontal red runtime.\n",
    "• The whole figure should match Figure 2 a qualitatively.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "9.  OPTIONAL IMPROVEMENTS / NOTES\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "•  Instead of hand-coded alternation use the vectorised numpy implementation –\n",
    "it is only ~15 lines of code.\n",
    "•  Add early stopping if λ stops moving.\n",
    "•  For numerical stability always subtract min(L̂) before exponentiation:\n",
    "ρ_i ∝ exp( −λ n (L̂_i – min(L̂)) ).\n",
    "•  For repeating the paper’s experiments on other datasets simply change n, r, m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65628b8",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Re-creation of Figure 2a (Ionosphere) from  \n",
    "“Thiemann, Igel, Wintenberger, Seldin 2017 –\n",
    " A Strongly Quasiconvex PAC-Bayesian Bound”.\n",
    "\n",
    "The script:\n",
    "\n",
    "1. loads the Ionosphere data set from UCI,\n",
    "2. builds the strong reference SVM tuned by 5-fold CV,\n",
    "3. builds a finite hypothesis set of m weak-SVMs\n",
    "   (each trained on r = d+1 random points),\n",
    "4. runs the alternating minimisation of the\n",
    "   PAC-Bayes-λ bound, obtains ρ and λ,\n",
    "5. evaluates the ρ-weighted majority vote on the\n",
    "   held-out test set, computes the PAC-Bayes-kl bound,\n",
    "6. draws Figure 2a.\n",
    "\n",
    "Only standard scientific-Python packages are used.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a792fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from scipy.special import logsumexp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0.  GLOBAL SETTINGS\n",
    "# ------------------------------------------------------------------\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "δ = 0.05  # confidence for PAC-Bayes\n",
    "m_grid = np.array([1, 2, 4, 8, 16, 32, 64, 128])  # x-axis in Fig 2a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef2cae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1.  DATA  (download once then keep in memory)\n",
    "# ------------------------------------------------------------------\n",
    "URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\"\n",
    "raw = urllib.request.urlopen(URL).read().decode()\n",
    "Xy = np.genfromtxt(io.StringIO(raw), delimiter=\",\", dtype=str)\n",
    "X_raw = Xy[:, :-1].astype(float)\n",
    "y_raw = np.where(Xy[:, -1] == \"g\", 1, -1)  # g → +1, b → –1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f6be82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (200, 34),  Test (150, 34),  d=34\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_raw)\n",
    "X_std = scaler.transform(X_raw)\n",
    "\n",
    "n_total, d = X_std.shape\n",
    "# fixed split (paper uses |S|=200, |T|=150)\n",
    "perm = np.random.permutation(n_total)\n",
    "S_idx = perm[:200]\n",
    "T_idx = perm[200:350]\n",
    "\n",
    "X_S, y_S = X_std[S_idx], y_raw[S_idx]\n",
    "X_T, y_T = X_std[T_idx], y_raw[T_idx]\n",
    "print(f\"Train {X_S.shape},  Test {X_T.shape},  d={d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d6321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 2.  STRONG CV-SVM  (baseline red horizontal lines)\n",
    "# ------------------------------------------------------------------\n",
    "def jaakkola_gamma(X, y):\n",
    "    G = np.full(len(X), np.inf)\n",
    "    for i, xi in enumerate(X):\n",
    "        mask = y != y[i]\n",
    "        dists = np.linalg.norm(xi - X[mask], axis=1)\n",
    "        G[i] = dists.min()\n",
    "    med = np.median(G)\n",
    "    return 1.0 / (2.0 * med**2)\n",
    "\n",
    "\n",
    "C_grid = 10.0 ** np.arange(-3, 4)\n",
    "γ_seed = jaakkola_gamma(X_S, y_S)\n",
    "γ_grid = γ_seed * 10.0 ** np.array([-4, -2, 0, 2, 4])\n",
    "\n",
    "param_grid = {\"C\": C_grid, \"gamma\": γ_grid}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=RNG_SEED)\n",
    "grid = GridSearchCV(SVC(kernel=\"rbf\"), param_grid, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "tic = time.perf_counter()\n",
    "grid.fit(X_S, y_S)\n",
    "t_cv = time.perf_counter() - tic\n",
    "\n",
    "svm_cv = grid.best_estimator_\n",
    "L_CV = zero_one_loss(y_T, svm_cv.predict(X_T))\n",
    "print(\"CV-SVM test loss:\", L_CV, \" time:\", t_cv, \"s\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  BUILD H  (weak SVMs)\n",
    "# ------------------------------------------------------------------\n",
    "r = d + 1  # 35 for ionosphere\n",
    "n_val = len(X_S) - r\n",
    "\n",
    "γ_candidates = γ_grid\n",
    "\n",
    "\n",
    "def train_weak_svm(train_idx):\n",
    "    γ = np.random.choice(γ_candidates)\n",
    "    clf = SVC(kernel=\"rbf\", C=1.0, gamma=γ)\n",
    "    clf.fit(X_S[train_idx], y_S[train_idx])\n",
    "    return clf, γ\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  PAC-BAYES HELPERS\n",
    "# ------------------------------------------------------------------\n",
    "def kl_inv(q, ε, tol=1e-10):\n",
    "    \"\"\"Return the smallest p in [q,1] s.t. kl(q||p) ≤ ε (binary search).\"\"\"\n",
    "    if ε <= 0:\n",
    "        return q\n",
    "    lo, hi = q, 1.0\n",
    "    while hi - lo > tol:\n",
    "        mid = (lo + hi) / 2\n",
    "        kl = q * np.log(q / mid) + (1 - q) * np.log((1 - q) / (1 - mid))\n",
    "        if kl > ε:\n",
    "            lo = mid\n",
    "        else:\n",
    "            hi = mid\n",
    "    return hi\n",
    "\n",
    "\n",
    "def pac_bayes_kl_bound(A, KL, n):\n",
    "    \"\"\"Invert kl to upper-bound the true loss (binary version).\"\"\"\n",
    "    ε = (KL + np.log(2 * np.sqrt(n) / δ)) / n\n",
    "    return kl_inv(A, ε)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  MAIN LOOP OVER m\n",
    "# ------------------------------------------------------------------\n",
    "loss_mv, loss_bound, time_pac = [], [], []\n",
    "\n",
    "for m in m_grid:\n",
    "    tic_total = time.perf_counter()\n",
    "\n",
    "    # 3.1  sample m subsets & train\n",
    "    weak_clfs, L_val = [], []\n",
    "    for _ in range(int(m)):\n",
    "        subset = np.random.choice(len(X_S), size=r, replace=False)\n",
    "        clf, _ = train_weak_svm(subset)\n",
    "        weak_clfs.append(clf)\n",
    "        mask_val = np.setdiff1d(np.arange(len(X_S)), subset, assume_unique=True)\n",
    "        y_pred = clf.predict(X_S[mask_val])\n",
    "        L_val.append(zero_one_loss(y_S[mask_val], y_pred))\n",
    "    L_val = np.array(L_val)\n",
    "\n",
    "    # 5.1  alternating minimisation\n",
    "    ρ = np.full(m, 1.0 / m)\n",
    "    λ = 0.5\n",
    "    for _ in range(20):\n",
    "        # ρ-update\n",
    "        logw = -λ * n_val * L_val + np.log(1.0 / m)\n",
    "        logw -= logsumexp(logw)\n",
    "        ρ = np.exp(logw)\n",
    "        KL = np.sum(ρ * (np.log(ρ) + np.log(m)))  # KL(ρ || uniform)\n",
    "\n",
    "        # λ-update  (Eq. 8)\n",
    "        A = np.sum(ρ * L_val)\n",
    "        B = KL + np.log(2 * np.sqrt(n_val) / δ)\n",
    "        λ_new = 2.0 / (np.sqrt(2 * n_val * A / B) + 1.0)\n",
    "        λ_new = np.clip(λ_new, np.sqrt(np.log(2 * np.sqrt(n_val) / δ) / n_val), 1.0)\n",
    "\n",
    "        if abs(λ - λ_new) < 1e-6:\n",
    "            λ = λ_new\n",
    "            break\n",
    "        λ = λ_new\n",
    "\n",
    "    # 6.  majority vote on test set\n",
    "    preds = np.zeros(len(X_T))\n",
    "    for clf, w in zip(weak_clfs, ρ):\n",
    "        preds += w * np.sign(clf.predict(X_T))\n",
    "    y_hat = np.where(preds > 0, 1, -1)\n",
    "    loss_mv.append(zero_one_loss(y_T, y_hat))\n",
    "\n",
    "    # 6.2 PAC-Bayes-kl bound on randomised classifier\n",
    "    A_emp = np.sum(ρ * L_val)\n",
    "    KL = np.sum(ρ * (np.log(ρ) + np.log(m)))\n",
    "    loss_bound.append(pac_bayes_kl_bound(A_emp, KL, n_val))\n",
    "\n",
    "    time_pac.append(time.perf_counter() - tic_total)\n",
    "    print(f\"m={m:4d}  λ={λ:.3f}  test-loss={loss_mv[-1]:.3f}  time={time_pac[-1]:.2f}s\")\n",
    "\n",
    "loss_mv = np.array(loss_mv)\n",
    "loss_bound = np.array(loss_bound)\n",
    "time_pac = np.array(time_pac)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7.  PLOT  (Figure 2a)\n",
    "# ------------------------------------------------------------------\n",
    "fig, ax1 = plt.subplots(figsize=(6.0, 4.0))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.set_xscale(\"log\", base=10)\n",
    "\n",
    "# dynamic y-limit so the blue bound is never clipped\n",
    "y_max = 1.10 * max(loss_mv.max(), loss_bound.max(), L_CV)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "ax1.plot(m_grid, loss_mv, \"k-\", label=\"Our Method\")  # solid black\n",
    "ax1.plot(m_grid, loss_bound, \"b--\", label=\"Bound\", linewidth=2)  # dashed blue\n",
    "ax1.axhline(L_CV, color=\"red\", label=\"CV SVM\")\n",
    "\n",
    "ax1.set_xlabel(\"m  (number of weak SVMs)\")\n",
    "ax1.set_ylabel(\"Zero-one test loss\")\n",
    "\n",
    "ax2.plot(m_grid, time_pac, \"k--\", label=r\"$t_m$\")  # <-- use time_pac\n",
    "ax2.axhline(t_cv, color=\"red\", linestyle=\"--\", label=r\"$t_{CV}$\")\n",
    "ax2.set_ylabel(\"Runtime (seconds)\")\n",
    "ax2.set_ylim(0, 1.20 * max(time_pac.max(), t_cv))\n",
    "\n",
    "# combined legend\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1 + h2, l1 + l2, loc=\"upper right\", fontsize=8)\n",
    "\n",
    "plt.title(\"Figure 2a reproduced with scikit-learn\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
